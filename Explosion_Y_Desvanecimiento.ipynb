{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from math import sqrt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a  \"simular\" una red neuronal completamente conectada con parámetros aleatorios. Digamos que x es la entrada y tenemos batch size de 64. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(64, 100) #64 vectores de tamaño 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0145), tensor(0.9855))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean(), x.std() #std es la desviación estandar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, en cada capa vamos a multiplicar por una matriz y aplicar una función de activación (e.g. relu) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# una red con 20 capas trono :( \n",
    "for i in range(20):\n",
    "    x @= torch.randn(100, 100)\n",
    "    torch.relu_(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wait! Exploto!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qué podemos hacer\n",
    "Hay varios cosas que se inventaron para tratar amellorar este problema y poder entrenar redes mucho más profundas: \n",
    "\n",
    "- 1. Inicializar correctamente las capas\n",
    "- 2. BatchNorm \n",
    "- 3. ResBlocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1077)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# En el caso de Inicializar correctamente las capas\n",
    "# Para que no truene debemos de multiplicar nuestro \n",
    "# vector por la raíz cuadrado de 2/100\n",
    "for i in range(50): \n",
    "    x @= torch.randn(100, 100)*sqrt(2/100) # Vuelve estable\n",
    "    # pasar por la red neuronal si inicializamos así\n",
    "    torch.relu_(x)\n",
    "\n",
    "x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Todo esto fue para redes neuronales completamente conectadas y activación de Relu**\n",
    "\n",
    "*Nota:* Si cambiamos relu por otra cosa, tendrémos que multiplicar por otra cosa para volverlo estable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batchnorm \n",
    "\n",
    "Esta es una idea muy sencilla. ¿Por qué no normalizar para tener desviación estándar 1 y media 0 después de cada capa? Pues eso es una de las dos cosas que intenta hacer batchnorm: \n",
    "\n",
    "x <- x-u/o\n",
    "\n",
    "Pero ¿cómo? En realidad no conocemos la desviación estándar y la media de x en la capa número 14. Entonces lo que hacemos es usar la desviación estándar y la media en la batch. \n",
    "\n",
    "**Nota:** \"u\" y \"o\" son vectores con el mismo número de canales que x sin contar la *batch_size*. Es decir, cada canal podría tener diferentes media y desviación. \n",
    "\n",
    "Para entonces nunca vas a poder predecir cosas con diferente media o desviación estándar!!\n",
    "\n",
    "Entonces lo que hace batchnorm es además decide, independientemente, dos parámetros uno multiplicativo (y (gama)) y uno aditivo (beta). Estos parámetros tienen el mismo número de canales que x (obviamente sin contar la batch). \n",
    "\n",
    "x <- yx+beta\n",
    "\n",
    "Casi siempre inicializamos y = (1,1,1;...,1) y beta = (0,0,0,...,0)\n",
    "\n",
    "En código: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = nn.BatchNorm1d(20) #1d para vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0604,  0.1279, -1.0824,  ...,  0.1797, -0.1209, -1.2498],\n",
       "        [-1.5454,  0.0953, -0.3157,  ..., -0.7441, -1.2516, -0.6801],\n",
       "        [ 0.9030,  1.1227,  1.5608,  ...,  0.9900,  1.3677,  0.5576],\n",
       "        ...,\n",
       "        [ 1.2803,  1.4097, -1.1554,  ..., -1.1472, -0.3524, -0.6955],\n",
       "        [ 1.3132, -1.2783,  0.2782,  ...,  0.3678,  0.2828,  0.7381],\n",
       "        [ 1.0917,  0.0105,  0.5509,  ..., -1.3673,  1.3939, -0.5661]],\n",
       "       grad_fn=<NativeBatchNormBackward>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(64, 20)\n",
    "B(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1.], requires_grad=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.weight #Estos números son parámetros, van aprendiendo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batchnorm básicamente quita el overfitting y el desvanecimiento de la gradiente. Por lo tanto usa Batchnorm en todas las redes que hagas Vidale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (2): ReLU()\n",
       "  (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Suponiendo que ya estás creando tu red, \n",
    "# debería de empezar algo así\n",
    "\n",
    "nn.Sequential(nn.BatchNorm2d(3), # iniciar con batchnorm\n",
    "              # para normalizar los datos es una buena \n",
    "              # práctica\n",
    "             nn.Conv2d(3,16,kernel_size=3), \n",
    "             nn.ReLU(), \n",
    "             nn.BatchNorm2d(16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resblocks \n",
    "\n",
    "Aún así, a las redes neuronales les cuesta trabajo aprender funciones sencillas (como por ejemplo la identidad). \n",
    "\n",
    "Entonces se les ocurrió que en vez de que cada \"capa\" calcule un valor, que más bien cada capa calcule \"un error\" o \"residuo\" (por eso residual nets). \n",
    "\n",
    "Es decir, que en vez de directamente calcular cuánto debe valer la salida, mejor que calcule cuánto hay que sumarle a la entrada para producir una buena salida: \n",
    "\n",
    "x -> f(x) *lo reemplazamos por* x -> x + f(x) \n",
    "\n",
    "Donde f podría ser una mini-red neuronal con unas 2 o 3 capas. \n",
    "\n",
    "De esta manera, los gradientes pueden \"brincar\" más sencillamente. \n",
    "\n",
    "O sea, en lugar de ir tipo: \n",
    "\n",
    "- PuntoA -> intermedio1 -> intermedio2 -> PuntoB\n",
    "\n",
    "van algo así: \n",
    "\n",
    "- PuntoA -> PuntoB\n",
    "\n",
    "En código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module): \n",
    "    def __init__(self, residual):\n",
    "        super().__init__()\n",
    "        self.residual = residual \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return (x + self.residual(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo lo usamos?\n",
    "\n",
    "Hay varias maneras de poner la red residual. Aquí hay una posibilidad, pero hay muchas. Experimenta!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_residual(filters): # Número de filtros de entrada \n",
    "    # número de filtros de x básciamente\n",
    "    bottleneck = (filters+1)//2\n",
    "    residual = nn.Sequential(\n",
    "        # Es como una compresión\n",
    "        nn.Conv2d(filters, bottleneck, kernel_size=1), #Esto es como para reducir el tamaño \n",
    "        nn.ReLU(), \n",
    "        nn.BatchNorm2d(bottleneck),\n",
    "        nn.Conv2d(bottleneck, bottleneck, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(bottleneck),\n",
    "        nn.Conv2d(bottleneck, filters, kernel_size=1),\n",
    "        nn.ReLU(),\n",
    "        # Es importante terminar con batchnorm\n",
    "        nn.BatchNorm2d(filters)\n",
    "    )\n",
    "    nn.init.constant_(residual[-1].weight, 0) \n",
    "    return (residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además este último batchnorm deberá estar inicializado con 0's (en vez de 1's) para y, como para que empiece siendo la identidad. Esto lo hacemos con nn.init.constant_. Porque quiero que empiece siendo la identidad y a partir de ahí aprenda cosas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResBlock(\n",
       "  (residual): Sequential(\n",
       "    (0): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ResBlock(crear_residual(16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya tenemos todos los ingredientes de resnet18, resnet34, etcétera!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai.vision.all as fv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fv.resnet18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
